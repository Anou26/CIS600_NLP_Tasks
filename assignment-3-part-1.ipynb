{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9801,"sourceType":"datasetVersion","datasetId":6763},{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Submitted By:**\n\n**Name: Anoushka Mergoju**\n\n**SUID: 328542442**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-14T23:53:07.761611Z","iopub.execute_input":"2024-04-14T23:53:07.762002Z","iopub.status.idle":"2024-04-14T23:53:08.938753Z","shell.execute_reply.started":"2024-04-14T23:53:07.761960Z","shell.execute_reply":"2024-04-14T23:53:08.937506Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\n/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\n/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nltk\n!pip install gensim","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:55:40.199685Z","iopub.execute_input":"2024-04-14T23:55:40.200814Z","iopub.status.idle":"2024-04-14T23:56:45.714173Z","shell.execute_reply.started":"2024-04-14T23:55:40.200767Z","shell.execute_reply":"2024-04-14T23:56:45.712611Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.4)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.4.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Part 1: Training a Word2Vec Model on the NLTK Movie Review Corpus**","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries and modules\nimport gensim\nimport nltk\nfrom nltk.corpus import movie_reviews\nfrom gensim.models import Word2Vec\nimport logging\n\n# Download nltk movie reviews corpus\nnltk.download('movie_reviews')\n\n#Reload the movie reviews variable\nmovie_reviews = nltk.corpus.movie_reviews\n\n# Enable logging to monitor training\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# Prepare data: List of lists of words\nsentences = [list(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()]\n\n# Train the Word2Vec model\nmodel = gensim.models.Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n\n# Save the model for later use\nmodel.save(\"movie_reviews_word2vec.model\")\nprint(\"Model training completed and saved.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:56:45.716650Z","iopub.execute_input":"2024-04-14T23:56:45.717136Z","iopub.status.idle":"2024-04-14T23:57:14.400809Z","shell.execute_reply.started":"2024-04-14T23:56:45.717098Z","shell.execute_reply":"2024-04-14T23:57:14.399573Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading movie_reviews: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\nModel training completed and saved.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Part 2: Testing the Model**","metadata":{}},{"cell_type":"markdown","source":"*Test 1: Find Top 5 Similar Words*","metadata":{}},{"cell_type":"code","source":"# Load the model\nmodel = Word2Vec.load(\"movie_reviews_word2vec.model\")\n\ntest_words = [\"movie\", \"star\", \"computer\", \"science\", \"king\", \"queen\", \"man\", \"woman\", \"dog\", \"cat\"]\nfor word in test_words:\n    try:\n        similar_words = model.wv.most_similar(word, topn=5)\n        print(f\"Words most similar to '{word}': {similar_words}\")\n    except KeyError:\n        print(f\"The word '{word}' is not in the vocabulary.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:59:00.300626Z","iopub.execute_input":"2024-04-14T23:59:00.301081Z","iopub.status.idle":"2024-04-14T23:59:00.563968Z","shell.execute_reply.started":"2024-04-14T23:59:00.301030Z","shell.execute_reply":"2024-04-14T23:59:00.557354Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Words most similar to 'movie': [('film', 0.9487786293029785), ('picture', 0.835985541343689), ('sequel', 0.7876085042953491), ('ending', 0.7412267923355103), ('story', 0.7386842966079712)]\nWords most similar to 'star': [('classic', 0.8083040714263916), ('trilogy', 0.8051865100860596), ('wars', 0.7903246879577637), ('episode', 0.7891380786895752), ('witch', 0.7834601998329163)]\nWords most similar to 'computer': [('generated', 0.8773682713508606), ('rom', 0.806863009929657), ('plots', 0.8039576411247253), ('domain', 0.8026084899902344), ('sub', 0.7935991287231445)]\nWords most similar to 'science': [('fiction', 0.93857741355896), ('pulp', 0.9123406410217285), ('horror', 0.8692793846130371), ('slasher', 0.8201507329940796), ('classic', 0.8167328238487244)]\nWords most similar to 'king': [('jerry', 0.8562236428260803), ('captain', 0.85112065076828), ('edward', 0.8398842215538025), ('george', 0.8381919860839844), ('jackson', 0.8328857421875)]\nWords most similar to 'queen': [('amidala', 0.9296970963478088), ('jay', 0.9293479323387146), ('ray', 0.9286536574363708), ('windsor', 0.9249902963638306), ('owner', 0.9195126295089722)]\nWords most similar to 'man': [('woman', 0.8921002745628357), ('girl', 0.840457558631897), ('boy', 0.8088302612304688), ('child', 0.7861366271972656), ('killer', 0.7463012933731079)]\nWords most similar to 'woman': [('girl', 0.9199714064598083), ('man', 0.8921003937721252), ('child', 0.8745524287223816), ('boy', 0.8487107753753662), ('doctor', 0.8222920894622803)]\nWords most similar to 'dog': [('dad', 0.8328391909599304), ('horse', 0.8179446458816528), ('boy', 0.8076778054237366), ('cat', 0.8060072660446167), ('conscience', 0.7945215106010437)]\nWords most similar to 'cat': [('blonde', 0.9101753234863281), ('tudeski', 0.9027919173240662), ('prowess', 0.9011302590370178), ('racist', 0.8995679020881653), ('shovel', 0.8950797319412231)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"*Test 2: Analogy Game*","metadata":{}},{"cell_type":"code","source":"analogies = [\n    (\"man\", \"woman\", \"king\"),\n    (\"paris\", \"france\", \"berlin\"),\n    (\"dog\", \"puppy\", \"cat\"),\n    (\"man\", \"woman\", \"programmer\")\n]\n\nfor a, b, c in analogies:\n    try:\n        result = model.wv.most_similar(positive=[c, b], negative=[a], topn=1)\n        print(f\"'{a}' is to '{b}' as '{c}' is to '{result[0][0]}'\")\n    except KeyError:\n        print(f\"An error occurred with the words: {a}, {b}, {c}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:59:58.386917Z","iopub.execute_input":"2024-04-14T23:59:58.387316Z","iopub.status.idle":"2024-04-14T23:59:58.405176Z","shell.execute_reply.started":"2024-04-14T23:59:58.387288Z","shell.execute_reply":"2024-04-14T23:59:58.403699Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"'man' is to 'woman' as 'king' is to 'jay'\n'paris' is to 'france' as 'berlin' is to '1963'\n'dog' is to 'puppy' as 'cat' is to 'impersonal'\n'man' is to 'woman' as 'programmer' is to 'fong'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Part 3: Using Pre-trained Google News Model**","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n# Load Google News model\ngoogle_model_path = '/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\ngoogle_model = KeyedVectors.load_word2vec_format(google_model_path, binary=True)\n\n# Repeat the tests\nfor word in test_words:\n    similar_words = google_model.most_similar(word, topn=5)\n    print(f\"Words most similar to '{word}' with Google News: {similar_words}\")\n\nfor a, b, c in analogies:\n    result = google_model.most_similar(positive=[c, b], negative=[a], topn=1)\n    print(f\"'{a}' is to '{b}' as '{c}' is to '{result[0][0]}' with Google News\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T00:01:14.426042Z","iopub.execute_input":"2024-04-15T00:01:14.426544Z","iopub.status.idle":"2024-04-15T00:02:21.943268Z","shell.execute_reply.started":"2024-04-15T00:01:14.426504Z","shell.execute_reply":"2024-04-15T00:02:21.942128Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Words most similar to 'movie' with Google News: [('film', 0.8676770329475403), ('movies', 0.8013108372688293), ('films', 0.7363011837005615), ('moive', 0.6830360889434814), ('Movie', 0.6693680286407471)]\nWords most similar to 'star' with Google News: [('stars', 0.7763954997062683), ('superstar', 0.7340598702430725), ('starlet', 0.6381064057350159), ('megastar', 0.6165120005607605), ('heart_throb', 0.5726701617240906)]\nWords most similar to 'computer' with Google News: [('computers', 0.7979379892349243), ('laptop', 0.6640493273735046), ('laptop_computer', 0.6548868417739868), ('Computer', 0.647333562374115), ('com_puter', 0.6082080006599426)]\nWords most similar to 'science' with Google News: [('faith_Jezierski', 0.6965422034263611), ('sciences', 0.6821076273918152), ('biology', 0.6775783896446228), ('scientific', 0.6535001993179321), ('mathematics', 0.6300910115242004)]\nWords most similar to 'king' with Google News: [('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474)]\nWords most similar to 'queen' with Google News: [('queens', 0.739944338798523), ('princess', 0.7070532441139221), ('king', 0.6510956883430481), ('monarch', 0.6383602023124695), ('very_pampered_McElhatton', 0.6357026696205139)]\nWords most similar to 'man' with Google News: [('woman', 0.7664012908935547), ('boy', 0.6824871301651001), ('teenager', 0.6586930155754089), ('teenage_girl', 0.6147903203964233), ('girl', 0.5921714305877686)]\nWords most similar to 'woman' with Google News: [('man', 0.7664012908935547), ('girl', 0.7494640946388245), ('teenage_girl', 0.7336829304695129), ('teenager', 0.6317085027694702), ('lady', 0.6288785934448242)]\nWords most similar to 'dog' with Google News: [('dogs', 0.8680489659309387), ('puppy', 0.8106428384780884), ('pit_bull', 0.780396044254303), ('pooch', 0.7627376914024353), ('cat', 0.7609457969665527)]\nWords most similar to 'cat' with Google News: [('cats', 0.8099379539489746), ('dog', 0.760945737361908), ('kitten', 0.7464985251426697), ('feline', 0.7326234579086304), ('beagle', 0.7150582671165466)]\n'man' is to 'woman' as 'king' is to 'queen' with Google News\n'paris' is to 'france' as 'berlin' is to 'germany' with Google News\n'dog' is to 'puppy' as 'cat' is to 'kitten' with Google News\n'man' is to 'woman' as 'programmer' is to 'programmers' with Google News\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Part 4: Using Pre-trained GloVe Model**","metadata":{}},{"cell_type":"code","source":"def glove_to_word2vec(glove_input_file, word2vec_output_file):\n    from gensim.scripts.glove2word2vec import glove2word2vec\n    glove2word2vec(glove_input_file, word2vec_output_file)\n\n# Call the function with the appropriate file paths\nglove_input_file = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'\nword2vec_output_file = 'glove.6B.100d.w2vformat.txt'\nglove_to_word2vec(glove_input_file, word2vec_output_file)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T00:04:48.416257Z","iopub.execute_input":"2024-04-15T00:04:48.416706Z","iopub.status.idle":"2024-04-15T00:05:42.607642Z","shell.execute_reply.started":"2024-04-15T00:04:48.416670Z","shell.execute_reply":"2024-04-15T00:05:42.606612Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/347223855.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n  glove2word2vec(glove_input_file, word2vec_output_file)\n","output_type":"stream"}]},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n# Load the converted model\nglove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T00:21:27.461817Z","iopub.execute_input":"2024-04-15T00:21:27.462350Z","iopub.status.idle":"2024-04-15T00:21:56.370648Z","shell.execute_reply.started":"2024-04-15T00:21:27.462315Z","shell.execute_reply":"2024-04-15T00:21:56.369501Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test_words = [\"movie\", \"star\", \"computer\", \"science\", \"king\", \"queen\", \"man\", \"woman\", \"dog\", \"cat\"]\nfor word in test_words:\n    try:\n        similar_words = glove_model.most_similar(word, topn=5)\n        print(f\"Words most similar to '{word}': {similar_words}\")\n    except KeyError:\n        print(f\"The word '{word}' is not in the vocabulary.\")\n\nanalogies = [\n    (\"man\", \"woman\", \"king\"),\n    (\"paris\", \"france\", \"berlin\"),\n    (\"dog\", \"puppy\", \"cat\"),\n    (\"man\", \"woman\", \"programmer\")\n]\n\nfor a, b, c in analogies:\n    try:\n        result = glove_model.most_similar(positive=[c, b], negative=[a], topn=1)\n        print(f\"'{a}' is to '{b}' as '{c}' is to '{result[0][0]}'\")\n    except KeyError:\n        print(f\"An error occurred with the words: {a}, {b}, {c}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T00:07:43.745025Z","iopub.execute_input":"2024-04-15T00:07:43.745519Z","iopub.status.idle":"2024-04-15T00:07:44.155829Z","shell.execute_reply.started":"2024-04-15T00:07:43.745488Z","shell.execute_reply":"2024-04-15T00:07:44.153999Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Words most similar to 'movie': [('film', 0.9055121541023254), ('movies', 0.8959327340126038), ('films', 0.866355299949646), ('hollywood', 0.8239826560020447), ('comedy', 0.8141382932662964)]\nWords most similar to 'star': [('stars', 0.8661765456199646), ('superstar', 0.728345513343811), ('movie', 0.6531304717063904), ('legend', 0.6483872532844543), ('actor', 0.6472946405410767)]\nWords most similar to 'computer': [('computers', 0.8751984238624573), ('software', 0.8373122215270996), ('technology', 0.7642159461975098), ('pc', 0.7366448640823364), ('hardware', 0.7290390729904175)]\nWords most similar to 'science': [('sciences', 0.8073161244392395), ('physics', 0.7914698123931885), ('institute', 0.7663252353668213), ('mathematics', 0.7607672810554504), ('studies', 0.7590447664260864)]\nWords most similar to 'king': [('prince', 0.7682328820228577), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775232315063), ('monarch', 0.6977890729904175)]\nWords most similar to 'queen': [('princess', 0.7947245240211487), ('king', 0.7507690191268921), ('elizabeth', 0.7355712056159973), ('royal', 0.7065026164054871), ('lady', 0.7044796943664551)]\nWords most similar to 'man': [('woman', 0.8323495388031006), ('boy', 0.7914870977401733), ('one', 0.7788748741149902), ('person', 0.7526816725730896), ('another', 0.7522234916687012)]\nWords most similar to 'woman': [('girl', 0.8472671508789062), ('man', 0.832349419593811), ('mother', 0.827568769454956), ('boy', 0.7720510363578796), ('she', 0.7632068395614624)]\nWords most similar to 'dog': [('cat', 0.8798074126243591), ('dogs', 0.8344309329986572), ('pet', 0.7449564337730408), ('puppy', 0.723637580871582), ('horse', 0.7109653949737549)]\nWords most similar to 'cat': [('dog', 0.8798074722290039), ('rabbit', 0.7424427270889282), ('cats', 0.732300341129303), ('monkey', 0.7288709878921509), ('pet', 0.719014048576355)]\n'man' is to 'woman' as 'king' is to 'queen'\n'paris' is to 'france' as 'berlin' is to 'germany'\n'dog' is to 'puppy' as 'cat' is to 'puppies'\n'man' is to 'woman' as 'programmer' is to 'educator'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Part 5: Model Comparison and Analysis**","metadata":{}},{"cell_type":"markdown","source":"Given the output from the three different models (trained on the NLTK Movie Review corpus, the pre-trained Google News Word2Vec, and the pre-trained GloVe model), let's analyze the performance of each based on the results:\n\n#### 1. **GloVe Model**\n   - **Similarity Task**: The GloVe model generally provides highly relevant similar words that align closely with our semantic expectations for the given words. It captures nuanced semantic relationships well, for instance, listing both \"hollywood\" and \"comedy\" as similar to \"movie,\" which reflects broader contextual understanding.\n   - **Analogy Task**: It performs standard analogy resolutions accurately, such as mapping \"man\" to \"woman\" as \"king\" to \"queen.\" However, it falters slightly with less straightforward analogies like predicting \"puppies\" instead of \"kitten\" for the analogy from \"dog\" to \"puppy\" and \"cat.\"\n\n#### 2. **Google News Word2Vec**\n   - **Similarity Task**: This model's strength is evident in its accurate and contextually relevant word similarities, reflecting its training on a vast and diverse dataset. However, it includes some noise (e.g., \"moive\" instead of \"movie\"), which might be a spelling error captured during its training.\n   - **Analogy Task**: Performs excellently on standard analogies and offers precise predictions that match closely with expected results, highlighting its effectiveness in understanding and processing real-world, frequently encountered relationships.\n\n#### 3. **NLTK Movie Reviews Word2Vec**\n   - **Similarity Task**: This model's outputs are more narrowly focused around movie-related content, which is a reflection of its training data. For example, the word \"science\" brings up genre-related words like \"fiction\" and \"horror.\"\n   - **Analogy Task**: It struggles with analogies, providing some bizarre or incorrect matches such as \"jay\" for the pair \"man\" to \"woman\" as \"king\" to \"jay\", which might be due to the limited and highly specialized training data.\n\n### Which Model Performs Best?\n\n- **For General Language Understanding and Breadth**: The **Google News Word2Vec model** stands out as the most robust in handling a wide variety of common and complex language tasks due to its extensive training on a large and diverse corpus. It provides accurate synonyms and resolves analogies with a high degree of reliability.\n\n- **For Context-Specific Tasks (Movies and Entertainment)**: The **NLTK Movie Reviews Word2Vec model** would be preferred if the domain of interest is strictly entertainment-related content, as it might capture nuanced sentiments or jargon specific to movie reviews better than the others.\n\n- **For a Balance of Nuance and Semantic Depth**: The **GloVe model** appears to strike a balance between breadth and depth, offering nuanced semantic connections that are particularly useful for tasks requiring a deep understanding of word relationships beyond mere co-occurrence.\n\n### Conclusion\n\nIn conclusion, the choice of the best model depends significantly on the specific needs of the application:\n- **Google News Word2Vec** is ideal for general-purpose NLP tasks.\n- **GloVe** offers deep semantic insights suitable for nuanced NLP applications.\n- **NLTK Movie Reviews Word2Vec** excels in domain-specific contexts where movie-related content is predominant.\n\nEach model's strengths and limitations reflect the nature of its training data and architecture, underscoring the importance of model selection based on the target application's specific requirements.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}